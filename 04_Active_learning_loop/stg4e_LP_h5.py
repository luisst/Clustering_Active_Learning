"""
Stage 4e: Label Propagation using HDF5 dataset with human labels.

This script performs label propagation using the merged_dataset_with_labels.h5
file generated by Stg4d_update_pickle_with_human_labels.py.

Key features:
1. Loads merged HDF5 dataset with human labels
2. Performs label propagation from human labels to unlabeled samples
3. Uses k-NN graph with MST for connectivity
4. Generates visualization and results CSV
"""

from pathlib import Path
import numpy as np
import pandas as pd
import h5py
import sys

from sklearn.neighbors import NearestNeighbors
from scipy.sparse import coo_matrix, csr_matrix
from scipy.sparse.csgraph import minimum_spanning_tree
from sklearn.preprocessing import normalize
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.lines as mlines
from collections import Counter
from scipy.stats import entropy

# ============================================================================
# HARDCODED CONFIGURATION
# ============================================================================
DATASET_NAME = 'TestAO-Irma'
STG3_METHOD = 'STG3_EXP010-SHAS-DV-hdb'
LP_METHOD = 'LP1'

# Paths
BASE_PATH = Path.home().joinpath('Dropbox', 'DATASETS_AUDIO', 'Unsupervised_Pipeline', DATASET_NAME)
STG4_FOLDER = BASE_PATH.joinpath('STG_4', f'STG4_{LP_METHOD}')

# Input: Updated HDF5 with human labels
MERGED_H5_WITH_LABELS = STG4_FOLDER / 'updated_h5_data' / 'merged_dataset_with_labels.h5'

# Output
OUTPUT_FOLDER = STG4_FOLDER / 'lp_results'
OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)

RUN_ID = 'RUN001'

# Label Propagation Parameters
K_NN = 5  # k-NN neighbors
METRIC = 'cosine'  # Distance metric
ADD_MST = True  # Add MST for connectivity
ALPHA = 0.5  # Propagation strength
MAX_ITER = 100  # Maximum iterations
TOL = 1e-6  # Convergence tolerance
WEAK_PRIOR_STRENGTH = 0.5  # Strength of HDBSCAN priors
MIN_CONFIDENCE_THRESHOLD = 0.7  # Minimum HDBSCAN confidence
MANUAL_ANCHOR_STRENGTH = 0.2  # Anchoring strength for manual labels
WEAK_ANCHOR_STRENGTH = 0.05  # Anchoring strength for HDBSCAN priors

# Animation/Visualization Parameters
SAVE_FRAMES = True  # Save animation frames
FRAME_INTERVAL = 5  # Save every Nth frame
CREATE_VIDEO = False  # Create MP4 video (requires ffmpeg)

# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def load_h5_with_labels(h5_path):
    """Load merged HDF5 dataset with human labels and recalculated features."""
    print(f"\nLoading HDF5 dataset: {h5_path}")

    with h5py.File(h5_path, 'r') as hf:
        n_samples = len(hf['merged_samples']['merged_unique_ids'])

        # Load merged sample data
        data = {
            'merged_unique_ids': [uid.decode() if isinstance(uid, bytes) else uid
                                 for uid in hf['merged_samples']['merged_unique_ids'][:]],
            'merged_cluster_labels_avgd': hf['merged_samples']['cluster_labels'][:],
            'cluster_probs': hf['merged_samples']['cluster_probs'][:],
            'gt_labels': hf['merged_samples']['gt_labels'][:],
        }

        # Load recalculated D-vector features
        if 'recalculated_features' in hf:
            data['dvectors'] = hf['recalculated_features']['dvectors'][:]
            print(f"  ✓ Loaded recalculated D-vectors: {data['dvectors'].shape}")
        else:
            raise ValueError("Recalculated features not found. Run Stage 3e first.")

        # Load Stage 3f clustering results (t-SNE, new HDBSCAN labels)
        if 'clustering_stage3f' in hf:
            data['tsne_2d'] = hf['clustering_stage3f']['tsne_2d'][:]
            data['cluster_labels_stage3f'] = hf['clustering_stage3f']['cluster_labels'][:]
            data['cluster_probs_stage3f'] = hf['clustering_stage3f']['cluster_probs'][:]
            print(f"  ✓ Loaded Stage 3f clustering results")
        else:
            raise ValueError("Stage 3f clustering results not found. Run Stage 3f first.")

        # Load human labels
        if 'human_labels' in hf:
            speaker_lp_bytes = hf['human_labels']['speaker_lp'][:]
            data['speaker_lp'] = [s.decode('utf-8') if s != b'' else None
                                 for s in speaker_lp_bytes]
            print(f"  ✓ Loaded human labels")
        else:
            raise ValueError("Human labels not found. Run Stage 4d first.")
        
        print(f"  Total samples: {n_samples}")

    return data


def build_knn_graph(X_data, k, metric='cosine'):
    """Build k-NN graph with local scaling."""
    print(f"\nBuilding k-NN graph (k={k}, metric={metric})...")

    n_samples = X_data.shape[0]
    nbrs = NearestNeighbors(n_neighbors=k+1, metric=metric, n_jobs=-1).fit(X_data)
    distances, indices = nbrs.kneighbors(X_data)

    # Drop self-connections
    distances = distances[:, 1:]
    indices = indices[:, 1:]

    # Compute local scale sigma_i
    sigma = distances[:, -1]
    sigma = np.maximum(sigma, 1e-8)

    # Build adjacency matrix with local scaling
    rows, cols, data = [], [], []

    for i in range(n_samples):
        for j_idx, d in zip(indices[i], distances[i]):
            if i == j_idx:
                continue
            wij = np.exp(-(d**2) / (sigma[i] * sigma[j_idx] + 1e-12))
            rows.append(i)
            cols.append(j_idx)
            data.append(wij)

    A = coo_matrix((data, (rows, cols)), shape=(n_samples, n_samples)).tocsr()

    # Symmetrize by intersection
    A_mutual = A.multiply(A.transpose())
    A = 0.5 * (A_mutual + A_mutual.transpose())

    # Add small self-connections
    A.setdiag(0.1)

    print(f"  ✓ k-NN graph built: {A.nnz} edges")

    return A, sigma


def add_mst_edges(A, X_data, sigma, metric='euclidean'):
    """Add MST edges for connectivity."""
    print(f"\nAdding MST for connectivity...")

    n_samples = X_data.shape[0]
    k = min(50, n_samples - 1)

    nbrs = NearestNeighbors(n_neighbors=k, metric=metric, n_jobs=-1).fit(X_data)
    distances, indices = nbrs.kneighbors(X_data)

    # Build sparse distance matrix
    rows, cols, data = [], [], []
    for i in range(n_samples):
        for j_idx, j in enumerate(indices[i]):
            if i != j:
                rows.append(i)
                cols.append(j)
                data.append(distances[i, j_idx])

    dist_matrix = csr_matrix((data, (rows, cols)), shape=(n_samples, n_samples))
    dist_matrix_sym = dist_matrix.minimum(dist_matrix.T)

    # Calculate MST
    mst = minimum_spanning_tree(dist_matrix_sym, overwrite=True)
    mst_coo = mst.tocoo()

    # Add MST edges
    for i, j, weight in zip(mst_coo.row, mst_coo.col, mst_coo.data):
        mst_weight = np.exp(-weight / np.mean(sigma))
        A[i, j] = max(A[i, j], mst_weight)
        A[j, i] = max(A[j, i], mst_weight)

    print(f"  ✓ Added {len(mst_coo.data)} MST edges")
    return A


def run_label_propagation(W, Y, human_labels, cluster_labels, cluster_probs,
                          cluster_to_speaker, id_to_idx, alpha, max_iter, tol,
                          manual_anchor_strength, weak_anchor_strength, min_confidence_threshold,
                          tsne_2d=None, output_folder=None, run_id='', save_frames=False, frame_interval=5):
    """
    Run label propagation with anchoring and optional frame saving.

    Parameters:
    -----------
    save_frames : bool
        Whether to save visualization frames during propagation
    frame_interval : int
        Save every Nth frame
    tsne_2d : np.ndarray
        t-SNE coordinates for visualization (required if save_frames=True)
    output_folder : Path
        Output folder for frames (required if save_frames=True)
    run_id : str
        Run identifier for naming frames

    Returns:
    --------
    F : np.ndarray
        Final propagated label distributions
    metrics : dict
        Dictionary with convergence history, confidence history, entropy history
    """
    print(f"\nRunning label propagation...")

    n_samples = Y.shape[0]
    n_classes = Y.shape[1]
    F = Y.copy()

    idx_to_id = {v: k for k, v in id_to_idx.items()}
    speaker_ids = list(id_to_idx.keys())

    # Track metrics
    convergence_history = []
    confidence_history = []
    entropy_history = []

    # Setup frames folder if needed
    frames_folder = None
    speaker_to_color = None
    if save_frames:
        if tsne_2d is None or output_folder is None:
            print("  Warning: Cannot save frames without tsne_2d and output_folder")
            save_frames = False
        else:
            frames_folder = output_folder / f"{run_id}_propagation_frames"
            frames_folder.mkdir(parents=True, exist_ok=True)
            print(f"  Frames will be saved to: {frames_folder}")

            # Color mapping for consistency
            palette = sns.color_palette('tab10', n_colors=len(speaker_ids))
            speaker_to_color = {spk: palette[i] for i, spk in enumerate(speaker_ids)}

    for it in range(max_iter):

        # Store previous state for convergence calculation
        F_prev = F.copy()

        # Propagation step
        F_new = alpha * W.dot(F)

        # Strong anchoring for manual labels
        for idx, spk in human_labels.items():
            speaker_idx = id_to_idx[spk]
            F_new[idx] = (1 - manual_anchor_strength) * F_new[idx]
            F_new[idx, speaker_idx] += manual_anchor_strength

        # Weak anchoring for HDBSCAN priors
        for i in range(n_samples):
            if i in human_labels:
                continue

            hdb_cluster = cluster_labels[i]
            hdb_confidence = cluster_probs[i]

            if (hdb_cluster != -1 and
                hdb_cluster in cluster_to_speaker and
                hdb_confidence >= min_confidence_threshold):

                speaker = cluster_to_speaker[hdb_cluster]
                speaker_idx = id_to_idx[speaker]

                anchor_weight = weak_anchor_strength * hdb_confidence
                F_new[i] = (1 - anchor_weight) * F_new[i]
                F_new[i, speaker_idx] += anchor_weight

        # Normalize rows
        row_sums = F_new.sum(axis=1, keepdims=True)
        row_sums[row_sums == 0] = 1
        F_new = F_new / row_sums

        # Calculate metrics
        delta = np.abs(F_new - F_prev).sum()
        convergence_history.append(delta)

        # Calculate average entropy (measure of uncertainty)
        eps = 1e-10
        sample_entropy = -np.sum(F_new * np.log(F_new + eps), axis=1)
        avg_entropy = np.mean(sample_entropy)
        entropy_history.append(avg_entropy)

        # Calculate average confidence
        confidence_scores = np.max(F_new, axis=1)
        avg_confidence = np.mean(confidence_scores)
        confidence_history.append(avg_confidence)

        F = F_new

        if (it + 1) % 10 == 0:
            print(f"  Iteration {it+1}/{max_iter}, delta={delta:.6f}, conf={avg_confidence:.4f}, entropy={avg_entropy:.4f}")

        # Save frame if needed
        if save_frames and (it % frame_interval == 0 or it == max_iter - 1):
            save_propagation_frame(
                tsne_2d, F, human_labels, speaker_to_color, id_to_idx, idx_to_id,
                convergence_history, confidence_history, entropy_history,
                it, max_iter, tol, frames_folder
            )

        if delta < tol:
            print(f"  ✓ Converged at iteration {it+1}")
            # Save final frame
            if save_frames and it % frame_interval != 0:
                save_propagation_frame(
                    tsne_2d, F, human_labels, speaker_to_color, id_to_idx, idx_to_id,
                    convergence_history, confidence_history, entropy_history,
                    it, max_iter, tol, frames_folder
                )
            break

    metrics = {
        'convergence_history': convergence_history,
        'confidence_history': confidence_history,
        'entropy_history': entropy_history,
        'frames_folder': frames_folder
    }

    return F, metrics


def save_propagation_frame(tsne_2d, F, human_labels, speaker_to_color, id_to_idx, idx_to_id,
                           conv_hist, conf_hist, ent_hist, iteration, max_iter, tol, frames_folder):
    """Save a single frame of the propagation visualization."""
    n_samples = F.shape[0]

    # Get current predictions
    y_current = np.argmax(F, axis=1)
    y_current_labels = np.array([idx_to_id[i] for i in y_current])
    confidence_scores = np.max(F, axis=1)

    # Create figure with subplots
    fig = plt.figure(figsize=(20, 10))
    gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)

    # Main plot: t-SNE with current predictions
    ax_main = fig.add_subplot(gs[:, :2])

    # Separate manual labels and propagated labels
    manual_indices = list(human_labels.keys())
    propagated_indices = [i for i in range(n_samples) if i not in manual_indices]

    # Plot propagated labels with transparency based on confidence
    for i in propagated_indices:
        conf = confidence_scores[i]
        color = speaker_to_color[y_current_labels[i]]
        ax_main.scatter(tsne_2d[i, 0], tsne_2d[i, 1],
                      c=[color], s=50, alpha=0.3 + 0.6*conf, edgecolors='none')

    # Plot manual labels with stars
    if manual_indices:
        colors_manual = [speaker_to_color[y_current_labels[i]] for i in manual_indices]
        ax_main.scatter(tsne_2d[manual_indices, 0], tsne_2d[manual_indices, 1],
                      c=colors_manual, s=200, alpha=1.0, marker='*',
                      edgecolors='black', linewidths=2, zorder=10)

    # Legend
    speaker_ids = list(speaker_to_color.keys())
    handles = [mlines.Line2D([], [], color=speaker_to_color[spk], marker='o',
                            linestyle='None', markersize=10, label=spk)
              for spk in speaker_ids]
    handles.append(mlines.Line2D([], [], color='gray', marker='*',
                                linestyle='None', markersize=15,
                                markeredgecolor='black', markeredgewidth=2,
                                label='Manual Labels'))
    ax_main.legend(handles=handles, title='Speakers', loc='best')
    ax_main.set_title(f'Label Propagation - Iteration {iteration+1}/{max_iter}',
                    fontsize=14, fontweight='bold')
    ax_main.set_xlabel('t-SNE 1')
    ax_main.set_ylabel('t-SNE 2')

    # Top right: Convergence plot
    ax_conv = fig.add_subplot(gs[0, 2])
    ax_conv.plot(conv_hist, linewidth=2, color='blue')
    ax_conv.axhline(y=tol, color='red', linestyle='--', label=f'Tolerance ({tol})')
    ax_conv.set_xlabel('Iteration')
    ax_conv.set_ylabel('Delta (Change)')
    ax_conv.set_title('Convergence')
    ax_conv.legend()
    ax_conv.grid(True, alpha=0.3)
    ax_conv.set_yscale('log')

    # Bottom right: Confidence and Entropy
    ax_metrics = fig.add_subplot(gs[1, 2])
    ax_metrics_twin = ax_metrics.twinx()

    line1 = ax_metrics.plot(conf_hist, linewidth=2, color='green', label='Avg Confidence')
    ax_metrics.set_xlabel('Iteration')
    ax_metrics.set_ylabel('Average Confidence', color='green')
    ax_metrics.tick_params(axis='y', labelcolor='green')

    line2 = ax_metrics_twin.plot(ent_hist, linewidth=2, color='orange', label='Avg Entropy')
    ax_metrics_twin.set_ylabel('Average Entropy', color='orange')
    ax_metrics_twin.tick_params(axis='y', labelcolor='orange')

    ax_metrics.set_title('Confidence & Entropy')
    ax_metrics.grid(True, alpha=0.3)

    # Info text
    info_text = f"Iteration: {iteration+1}/{max_iter}\n"
    info_text += f"Delta: {conv_hist[-1]:.6f}\n"
    info_text += f"Avg Confidence: {conf_hist[-1]:.4f}\n"
    info_text += f"Avg Entropy: {ent_hist[-1]:.4f}\n"
    info_text += f"Manual Labels: {len(human_labels)}"

    fig.text(0.02, 0.98, info_text, transform=fig.transFigure,
            fontsize=10, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    # Save frame
    frame_path = frames_folder / f"frame_{iteration:04d}.png"
    plt.savefig(frame_path, dpi=100, bbox_inches='tight')
    plt.close(fig)


# ============================================================================
# MAIN EXECUTION
# ============================================================================

print("="*80)
print("STAGE 4E: LABEL PROPAGATION WITH HDF5")
print("="*80)
print(f"Dataset: {DATASET_NAME}")
print(f"STG3 Method: {STG3_METHOD}")
print(f"LP Method: {LP_METHOD}")
print(f"Input HDF5: {MERGED_H5_WITH_LABELS}")
print(f"Output folder: {OUTPUT_FOLDER}")
print("="*80)

# Validate input
if not MERGED_H5_WITH_LABELS.exists():
    print(f"\nError: HDF5 file not found: {MERGED_H5_WITH_LABELS}")
    print("Please run Stage 4d first to create the file with human labels.")
    sys.exit(1)

# Load HDF5 data
data = load_h5_with_labels(MERGED_H5_WITH_LABELS)

# Extract arrays
X_data = data['dvectors']
tsne_2d = data['tsne_2d']
cluster_labels = data['cluster_labels_stage3f']  # Use Stage 3f clusters
cluster_probs = data['cluster_probs_stage3f']
gt_labels = data['gt_labels']
speaker_lp_list = data['speaker_lp']
unique_ids = data['merged_unique_ids']

n_samples = len(unique_ids)

# Convert speaker labels to dictionary
human_labels = {}
for idx, speaker in enumerate(speaker_lp_list):
    if speaker is not None:
        human_labels[idx] = speaker

print(f"\nHuman labels:")
print(f"  Labeled samples: {len(human_labels)}")
print(f"  Unlabeled samples: {n_samples - len(human_labels)}")
print(f"  Speaker IDs: {sorted(set(human_labels.values()))}")

if len(human_labels) == 0:
    print("\nError: No human labels found. Please run Stage 4d first and provide human labels.")
    sys.exit(1)

# Build k-NN graph
A, sigma = build_knn_graph(X_data, K_NN, METRIC)

# Add MST
A = add_mst_edges(A, X_data, sigma, metric='euclidean')

# Normalize adjacency matrix
W = normalize(A.tocsr(), norm='l1', axis=1)
print(f"  ✓ Adjacency matrix normalized")

# Initialize label distributions
print(f"\n{'='*80}")
print("INITIALIZING LABEL DISTRIBUTIONS")
print("="*80)

speaker_ids = sorted(set(human_labels.values()))
id_to_idx = {spk: idx for idx, spk in enumerate(speaker_ids)}
idx_to_id = {v: k for k, v in id_to_idx.items()}
n_classes = len(speaker_ids)

# Create cluster-to-speaker mapping
cluster_to_speaker = {}
for idx, spk in human_labels.items():
    hdb_cluster = cluster_labels[idx]
    if hdb_cluster != -1:
        if hdb_cluster in cluster_to_speaker:
            if cluster_to_speaker[hdb_cluster] != spk:
                print(f" >>> Warning: Cluster {hdb_cluster} has conflicting labels")
        cluster_to_speaker[hdb_cluster] = spk

print(f"  Cluster-to-speaker mapping: {cluster_to_speaker}")

# Initialize label matrix
Y = np.zeros((n_samples, n_classes))

# Strong priors for manual labels
for idx, spk in human_labels.items():
    Y[idx, id_to_idx[spk]] = 1.0

# Weak priors for HDBSCAN predictions
weak_priors_count = 0
for i in range(n_samples):
    if i in human_labels:
        continue

    hdb_cluster = cluster_labels[i]
    hdb_confidence = cluster_probs[i]

    # Only add weak prior if:
    # 1. Not noise (-1)
    # 2. Cluster is mapped to a speaker
    # 3. HDBSCAN confidence is above threshold
    if (hdb_cluster != -1 and
        hdb_cluster in cluster_to_speaker and
        hdb_confidence >= MIN_CONFIDENCE_THRESHOLD):

        speaker = cluster_to_speaker[hdb_cluster]
        speaker_idx = id_to_idx[speaker]

        # Set weak prior proportional to HDBSCAN confidence
        prior_strength = WEAK_PRIOR_STRENGTH * hdb_confidence
        Y[i, speaker_idx] = prior_strength

        # Distribute remaining probability uniformly among other classes
        remaining_prob = 1.0 - prior_strength
        uniform_prob = remaining_prob / (n_classes - 1)
        for j in range(n_classes):
            if j != speaker_idx:
                Y[i, j] = uniform_prob

        weak_priors_count += 1

print(f"  Manually labeled: {len(human_labels)}")
print(f"  HDBSCAN weak priors: {weak_priors_count}")
print(f"  Speaker classes: {n_classes}")

# Run label propagation
print(f"\n{'='*80}")
print("RUNNING LABEL PROPAGATION")
print("="*80)

F, metrics = run_label_propagation(
    W, Y, human_labels, cluster_labels, cluster_probs,
    cluster_to_speaker, id_to_idx, ALPHA, MAX_ITER, TOL,
    MANUAL_ANCHOR_STRENGTH, WEAK_ANCHOR_STRENGTH, MIN_CONFIDENCE_THRESHOLD,
    tsne_2d=tsne_2d, output_folder=OUTPUT_FOLDER, run_id=RUN_ID,
    save_frames=SAVE_FRAMES, frame_interval=FRAME_INTERVAL
)

# Get final predictions
y_pred_idx = np.argmax(F, axis=1)
y_pred = np.array([idx_to_id[i] for i in y_pred_idx])
confidence_scores = np.max(F, axis=1)

# Map numeric GT labels to speaker names using human labels
print(f"\n{'='*80}")
print("MAPPING GROUND TRUTH LABELS TO SPEAKER NAMES")
print("="*80)

# Build mapping from numeric GT labels to speaker names
gt_label_to_speaker = {}
for sample_idx, speaker in human_labels.items():
    numeric_gt = gt_labels[sample_idx]
    if numeric_gt not in gt_label_to_speaker:
        gt_label_to_speaker[numeric_gt] = speaker
    elif gt_label_to_speaker[numeric_gt] != speaker:
        print(f"  Warning: GT label {numeric_gt} has conflicting speaker assignments")

print(f"  GT label to speaker mapping: {gt_label_to_speaker}")

# Create mapped GT labels (convert numeric to speaker names)
gt_labels_mapped = []
unmapped_gt_labels = set()
for numeric_gt in gt_labels:
    if numeric_gt in gt_label_to_speaker:
        gt_labels_mapped.append(gt_label_to_speaker[numeric_gt])
    else:
        # Unknown GT label - use placeholder
        gt_labels_mapped.append('UNKNOWN')
        unmapped_gt_labels.add(numeric_gt)

gt_labels_mapped = np.array(gt_labels_mapped)

if unmapped_gt_labels:
    print(f"  Warning: {len(unmapped_gt_labels)} GT labels could not be mapped: {sorted(unmapped_gt_labels)}")
    print(f"  These samples will be labeled as 'UNKNOWN'")

print(f"\n{'='*80}")
print("PROPAGATION RESULTS")
print("="*80)
print(f"  Average confidence: {np.mean(confidence_scores):.4f}")
print(f"  Min confidence: {np.min(confidence_scores):.4f}")
print(f"  Max confidence: {np.max(confidence_scores):.4f}")

# Label distribution
pred_counts = Counter(y_pred)
print(f"\n  Label distribution after propagation:")
for spk, count in sorted(pred_counts.items()):
    print(f"    {spk}: {count} samples")

# Save results
results_df = pd.DataFrame({
    'merged_unique_id': unique_ids,
    'gt_label': gt_labels,
    'hdbscan_label': cluster_labels,
    'hdbscan_prob': cluster_probs,
    'lp_label': y_pred,
    'lp_confidence': confidence_scores,
    'human_label': [human_labels.get(i, None) for i in range(n_samples)]
})

results_csv_path = OUTPUT_FOLDER / f"{RUN_ID}_lp_results.csv"
results_df.to_csv(results_csv_path, index=False)
print(f"\n✓ Results saved to: {results_csv_path}")

# Generate visualization
print(f"\n{'='*80}")
print("GENERATING VISUALIZATION")
print("="*80)

fig, axes = plt.subplots(1, 3, figsize=(24, 8))

# Plot 1: Ground Truth labels (mapped to speaker names)
ax = axes[0]
gt_counts = Counter(gt_labels_mapped)
unique_gt = np.unique(gt_labels_mapped)
palette_gt = sns.color_palette('tab10', n_colors=len(unique_gt))
gt_to_color = {label: palette_gt[i] for i, label in enumerate(unique_gt)}
colors_gt = [gt_to_color[label] for label in gt_labels_mapped]
ax.scatter(tsne_2d[:, 0], tsne_2d[:, 1], c=colors_gt, s=50, alpha=0.7)
handles = [mlines.Line2D([], [], color=gt_to_color[label], marker='o', linestyle='None',
                         markersize=8, label=f"{label} (n={gt_counts[label]})")
           for label in unique_gt]
ax.legend(handles=handles, title='Ground Truth', loc='best')
ax.set_title('Ground Truth Labels')
ax.set_xlabel('t-SNE 1')
ax.set_ylabel('t-SNE 2')

# Plot 2: HDBSCAN clusters
ax = axes[1]
hdbscan_counts = Counter(cluster_labels)
unique_hdbscan = np.unique(cluster_labels)
palette_hdb = sns.color_palette('tab10', n_colors=len(unique_hdbscan))
hdb_to_color = {label: palette_hdb[i] for i, label in enumerate(unique_hdbscan)}
colors_hdb = [hdb_to_color[label] for label in cluster_labels]
ax.scatter(tsne_2d[:, 0], tsne_2d[:, 1], c=colors_hdb, s=50, alpha=0.7)
handles = [mlines.Line2D([], [], color=hdb_to_color[label], marker='o', linestyle='None',
                         markersize=8, label=f"C{label} (n={hdbscan_counts[label]})" if label != -1 else f"Noise (n={hdbscan_counts[label]})")
           for label in unique_hdbscan]
ax.legend(handles=handles, title='HDBSCAN (Stage 3f)', loc='best')
ax.set_title('HDBSCAN Clustering Results')
ax.set_xlabel('t-SNE 1')
ax.set_ylabel('t-SNE 2')

# Plot 3: Label propagation results
ax = axes[2]
lp_counts = Counter(y_pred)
unique_lp = np.unique(y_pred)
palette_lp = sns.color_palette('tab10', n_colors=len(unique_lp))
lp_to_color = {label: palette_lp[i] for i, label in enumerate(unique_lp)}

# Plot unlabeled samples with transparency based on confidence
unlabeled_indices = [i for i in range(n_samples) if i not in human_labels]
labeled_indices = list(human_labels.keys())

for i in unlabeled_indices:
    conf = confidence_scores[i]
    ax.scatter(tsne_2d[i, 0], tsne_2d[i, 1],
              c=[lp_to_color[y_pred[i]]], s=50, alpha=0.3 + 0.6*conf, edgecolors='none')

# Plot labeled samples with stars
if labeled_indices:
    ax.scatter(tsne_2d[labeled_indices, 0], tsne_2d[labeled_indices, 1],
              c=[lp_to_color[y_pred[i]] for i in labeled_indices],
              s=200, alpha=1.0, marker='*', edgecolors='black', linewidths=2, zorder=10)

handles = [mlines.Line2D([], [], color=lp_to_color[label], marker='o', linestyle='None',
                         markersize=8, label=f"{label} (n={lp_counts[label]})") for label in unique_lp]
handles.append(mlines.Line2D([], [], color='gray', marker='*', linestyle='None',
                            markersize=15, markeredgecolor='black', markeredgewidth=2,
                            label='Manual Labels'))
ax.legend(handles=handles, title='Label Propagation', loc='best')
ax.set_title(f'Label Propagation Results ({len(human_labels)} manual labels)')
ax.set_xlabel('t-SNE 1')
ax.set_ylabel('t-SNE 2')

plt.tight_layout()
plot_path = OUTPUT_FOLDER / f"{RUN_ID}_gt_hdb_lp_comparison.png"
plt.savefig(plot_path, dpi=300, bbox_inches='tight')
print(f"✓ Visualization saved to: {plot_path}")
plt.show()

# Calculate accuracy metrics against Ground Truth
print(f"\n{'='*80}")
print("CALCULATING ACCURACY METRICS")
print("="*80)

gt_accuracy = accuracy_score(gt_labels_mapped, y_pred)
print(f"  Overall accuracy vs GT: {gt_accuracy*100:.2f}%")

# Classification report
unique_labels_all = np.unique(np.concatenate([gt_labels_mapped, y_pred]))
print(f"  Unique labels: {sorted(unique_labels_all)}")

try:
    class_report = classification_report(gt_labels_mapped, y_pred, labels=unique_labels_all, zero_division=0)
    print(f"  Classification Report:\n{class_report}")
except Exception as e:
    print(f"  Could not generate classification report: {e}")
    class_report = "Error generating report"

# Confusion matrix
try:
    conf_matrix = confusion_matrix(gt_labels_mapped, y_pred, labels=unique_labels_all)
    print(f"  Confusion Matrix:")
    print(f"  GT\\Pred: {unique_labels_all}")
    for i, gt_label in enumerate(unique_labels_all):
        print(f"  {gt_label}: {conf_matrix[i]}")
except Exception as e:
    print(f"  Could not generate confusion matrix: {e}")
    conf_matrix = None

# Per-class accuracy
class_accuracies = {}
for label in np.unique(gt_labels_mapped):
    mask = gt_labels_mapped == label
    if np.sum(mask) > 0:
        class_acc = accuracy_score(gt_labels_mapped[mask], y_pred[mask])
        class_accuracies[label] = class_acc
        print(f"  Class '{label}' accuracy: {class_acc*100:.2f}% ({np.sum(mask)} samples)")

# Save log
log_path = OUTPUT_FOLDER / f"{RUN_ID}_lp_log.txt"
with open(log_path, 'w', encoding='utf-8') as f:
    f.write(f"LABEL PROPAGATION LOG - {RUN_ID}\n")
    f.write(f"{'='*80}\n")
    f.write(f"Dataset: {DATASET_NAME}\n")
    f.write(f"Total samples: {n_samples}\n")
    f.write(f"Manually labeled: {len(human_labels)}\n")
    f.write(f"Speaker IDs: {speaker_ids}\n")

    f.write(f"\n{'='*80}\n")
    f.write(f"PARAMETERS\n")
    f.write(f"{'='*80}\n")
    f.write(f"  k-NN: {K_NN}\n")
    f.write(f"  Metric: {METRIC}\n")
    f.write(f"  Alpha: {ALPHA}\n")
    f.write(f"  Max iterations: {MAX_ITER}\n")
    f.write(f"  Manual anchor strength: {MANUAL_ANCHOR_STRENGTH}\n")
    f.write(f"  Weak anchor strength: {WEAK_ANCHOR_STRENGTH}\n")
    f.write(f"  Min confidence threshold: {MIN_CONFIDENCE_THRESHOLD}\n")
    f.write(f"  Weak prior strength: {WEAK_PRIOR_STRENGTH}\n")

    f.write(f"\n{'='*80}\n")
    f.write(f"CONVERGENCE METRICS\n")
    f.write(f"{'='*80}\n")
    f.write(f"  Iterations run: {len(metrics['convergence_history'])}\n")
    f.write(f"  Final delta: {metrics['convergence_history'][-1]:.6f}\n")
    f.write(f"  Final avg confidence: {metrics['confidence_history'][-1]:.4f}\n")
    f.write(f"  Final avg entropy: {metrics['entropy_history'][-1]:.4f}\n")

    f.write(f"\n{'='*80}\n")
    f.write(f"RESULTS\n")
    f.write(f"{'='*80}\n")
    f.write(f"  Avg confidence: {np.mean(confidence_scores):.4f}\n")
    f.write(f"  Min confidence: {np.min(confidence_scores):.4f}\n")
    f.write(f"  Max confidence: {np.max(confidence_scores):.4f}\n")
    f.write(f"  Label distribution:\n")
    for spk, count in sorted(pred_counts.items()):
        f.write(f"    {spk}: {count}\n")

    f.write(f"\n{'='*80}\n")
    f.write(f"GROUND TRUTH COMPARISON\n")
    f.write(f"{'='*80}\n")
    f.write(f"  Overall accuracy vs GT: {gt_accuracy*100:.2f}%\n")
    f.write(f"  Unique GT labels: {sorted(np.unique(gt_labels_mapped).tolist())}\n")
    f.write(f"  Unique predicted labels: {sorted(np.unique(y_pred).tolist())}\n")
    f.write(f"  GT label mapping: {gt_label_to_speaker}\n")

    f.write(f"\n  Per-class accuracy:\n")
    for label, acc in class_accuracies.items():
        gt_count = np.sum(gt_labels_mapped == label)
        f.write(f"    Class '{label}': {acc*100:.2f}% ({gt_count} samples)\n")

    f.write(f"\n  Classification Report:\n")
    f.write(class_report)
    f.write(f"\n")

    if conf_matrix is not None:
        f.write(f"\n  Confusion Matrix:\n")
        f.write(f"  GT\\Pred: {unique_labels_all}\n")
        for i, gt_label in enumerate(unique_labels_all):
            f.write(f"  {gt_label}: {conf_matrix[i]}\n")

    # HDBSCAN clustering info
    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
    percentage_assigned = (np.sum(cluster_labels != -1) / n_samples) * 100
    f.write(f"\n{'='*80}\n")
    f.write(f"HDBSCAN CLUSTERING INFO\n")
    f.write(f"{'='*80}\n")
    f.write(f"  Number of clusters: {n_clusters}\n")
    f.write(f"  Percentage assigned: {percentage_assigned:.2f}%\n")
    f.write(f"  Cluster-to-speaker mapping:\n")
    for cluster_id, speaker in cluster_to_speaker.items():
        f.write(f"    Cluster {cluster_id} -> {speaker}\n")

    # Detailed per-sample predictions
    f.write(f"\n{'='*80}\n")
    f.write(f"DETAILED PREDICTIONS\n")
    f.write(f"{'='*80}\n")
    for idx, unique_id in enumerate(unique_ids):
        gt_numeric = gt_labels[idx]
        gt_mapped = gt_labels_mapped[idx]
        hdb = cluster_labels[idx]
        hdbp = cluster_probs[idx]
        lp = y_pred[idx]
        lpc = confidence_scores[idx]
        human = 'YES' if idx in human_labels else 'NO'
        f.write(f"{unique_id}: GT={gt_mapped} ({gt_numeric}), HDBSCAN=C{hdb} (p={hdbp:.2f}), LP={lp} (conf={lpc:.2f}), Manual={human}\n")

print(f"✓ Log saved to: {log_path}")

print(f"\n{'='*80}")
print("PROCESS COMPLETED SUCCESSFULLY!")
print("="*80)
print(f"Output Files:")
print(f"  - Results CSV: {results_csv_path}")
print(f"  - Visualization: {plot_path}")
print(f"  - Log file: {log_path}")
if SAVE_FRAMES and metrics['frames_folder']:
    n_frames = len(list(metrics['frames_folder'].glob('*.png')))
    print(f"  - Animation frames: {metrics['frames_folder']} ({n_frames} frames)")
print(f"\nSummary:")
print(f"  - Samples: {n_samples} total, {len(human_labels)} manually labeled")
print(f"  - Accuracy vs GT: {gt_accuracy*100:.2f}%")
print(f"  - Avg confidence: {np.mean(confidence_scores):.4f}")
print(f"  - Converged in {len(metrics['convergence_history'])} iterations")
print("="*80)
